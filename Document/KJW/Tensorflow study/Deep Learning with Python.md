# Deep Learning with Python

- `용어가 책마다 다르고 설명의 깊이가 모두 달라 필요 부분과 그 이전 책들과 다른 부분 정리하면서 읽음.`
- 한국어판 이름은 케라스 창시자에게 배우는 딥러닝.
- 예제 코드가 모두 https://keras.io 를 바탕으로 이뤄짐. 하지만 우리는 tensorflow를 활용하니 https://www.tensorflow.org/api_docs/python/tf/keras 를 참고하며 공부함.
- 예제 코드
  - 길벗출판사 깃허브: https://github.com/gilbutITbook/006975
  - 역자 깃허브: https://github.com/rickiepark/deep-learning-with-python-notebooks
  - 위에 두 곳은 keras만 사용한듯 하며 우리는 tensorflow를 통해서 keras를 쓸 예정이니 아래 코드 참고.
    - tf.keras 포팅 버전: https://github.com/rickiepark/deep-learning-with-python-notebooks/tree/tf2
    - `포팅(porting)`은 컴퓨터 과학에서 실행 가능한 프로그램이 원래 설계된 바와 다른 컴퓨팅 환경(이를테면 CPU, 운영 체제, 서드 파티 라이브러리 등)에서 동작할 수 있도록 하는 과정을 가리킴.
    - python 3.6, tensorflow 2.3 사용. 



### chapter1 딥러닝이란?

#### 패러다임 변화 과정

- `인공지능 > 머신러닝 > 딥러닝`

<img src="C:\Users\multicampus\Documents\s03p31c203\Document\KJW\Tensorflow study\Deep Learning with Python.assets\book2-1.jpg" alt="book2-1" style="zoom: 25%;" />

- 프로그래머들이 명시적인 `규칙을 충분하게 많이 만들어(하드코딩)` 지식을 다루면 인공 지능을 만들 수 있다고 믿었음. 이런 접근 방식을 `심볼릭 AI(symbolic AI)` 라고 함.
- 1980년대까지 AI 분야의 지배적인 패러다임이였음. 체스 게임처럼 잘 정의된 논리적인 문제를 푸는 데 적합하다는 게 증명되었지만 이미지 분류, 음성 인식, 언어 번역 같은 더 복잡하고 불분명한 문제를 해결하기 위한 명확한 규칙을 찾는데는 한계를 가짐. `symbolic AI`를 대체하기 위해 `머신러닝`이 등장함. 
- 전통적인 프로그래밍인 `심볼릭 AI`는 `규칙(프로그램)`과 이 규칙에 따라 처리될 `데이터`를 입력하면 `해답`이 출력됌. 하지만 `머신러닝`에서는 `데이터`와 이 데이터로부터 기대되는 `해답`을 입력하면 `규칙이 출력됌`. 이 규칙을 `새로운 데이터에 적용`하여 `창의적인 답`을 만들 수 있음.
- 1990년 대부터 고성능 하드웨어와 대량의 데이터셋이 가능해지면서 AI에서 가장 인기있는 분야가 됌. 
- 머신러닝은 수리통계와 관련되어 있지만 다름.
  - 머신 러닝은 통계와 달리 보통 대량의 복잡한 데이터셋(ex. 몇 만개의 픽셀로 구성된 이미지가 수백만 개가 있는 데이터셋)을 다루기 때문에 베이지안 분석(Bayesian analysis) 같은 전통적인 통계 분석 방법은 적용하기 힘듬. 
  - 이런 이유로 머신러닝, 특히 딥러닝은 수학적 이론이 비교적 부족하고 `엔지니어링 지향적`임. 이런 실천적인 접근 방식 때문에 이론보다는 `경험을 바탕으로 아이디어가 증명되는 경우가 많음`. 
    - 딥러닝을 연금술에 비유한 구글의 Ali Rahimi (알리 라히미)와 Yann LeCun(얀 르쿤) 박사의 설전.
    - https://www.youtube.com/watch?v=Qi1Yry33TQE&ab_channel=ObservantVids



#### 머신러닝(Machine Learning)

- 머신러닝: 샘플과 기댓값이 주어졌을 때 데이터 처리 작업을 위한 실행 규칙을 찾는 것. 그리고 이런 규칙을 바탕으로 머신 러닝 모델이 입력 데이터를 의미 있는 출력으로 변환함. 

  - 머신 러닝과 딥러닝의 핵심 문제는 `의미 있는 데이터로의 변환`. 즉, 입력 데이터를 기반으로 기대 출력에 가깝게 만드는 유용한 `표현(representation)`을 `학습(training)`하는 것.
    - 표현: 데이터를 인코딩하거나 묘사하기 위해 데이터를 바라보는 방법.
  - 머신 러닝에서의 `학습`이란 `더 나은 표현`을 찾는 자동화된 과정.

- 머신러닝을 하기 위해 필요한 세 가지

  - `입력 데이터 포인트`
    - 문제가 음성인식이라면 데이터 포인트는 `음성`, 이미지 태깅이라면 데이터 포인트는 `이미지`
    - 머신러닝 `분류 문제`의 `범주(category)`를 `클래스(class)`라고 하고 데이터 포인트는 `샘플(sample)`을 의미함. 특정 샘플의 클래스를 `레이블(label)`이라고 함. 
  - `기대 출력`
    - 음성 인식 작업에서 기대출력은 사람이 사운드 파일을 듣고 `옮긴 글`, 이미지 작업에서는 강아지,고양이 같은 `태그`
  - `알고리즘의 성능을 측정하는 방법`
    - 알고리즘의 `현재 출력`과 `기대 출력` 간의 `차이`를 결정하기 위해 필요함. 측정값은 알고리즘의 작동 방식을 교정하기 위한 신호로 다시 `피드백` 됌. 이런 수정단계를 `학습(learning)`이라고 함.

  

#### 딥러닝(Deep Learning)

- 딥러닝: `머신 러닝의 한 분야`로서 연속된 `층(layer)`에서 점진적으로 의미 있는 표현을 배우는 데 강점이 있는, 데이터로부터 `표현`을 학습하는 새로운 방식. 
- Deep Learning에서 `Deep`이란 단어가 어떤 깊은 통창을 얻을 수 있다는 것을 의미하지는 않음. `그냥 연속된 층`으로 표현을 학습한다는 개념을 나타냄.
  - 데이터로부터 모델을 만드는 데 얼마나 많은 층을 사용했는지가 그 모델의 깊이가 됌. 
- 딥러닝에서는 기본 층을 겹겹이 쌓아 올려 구성한 `신경망(neural network)`이라는 모델을 사용하여 표현 층을 학습함. 
  - 대중 과학 저널에서 딥러닝이 뇌처럼 작동한다거나 뇌를 모방하여 만들었다고 주장하는 글을 볼 수 있지만, 이는 사실이 아님. 
  - 딥러닝의 일부 핵심 개념이 뇌 구조를 이해하는 것에서 부터 영감을 얻어 개발된 부분은 있지만 딥러닝 모델이 뇌를 모델링한 것은 아님.
  - 최근의 딥러닝 모델이 사용하는 학습 메커니즘과 유사한 것을 뇌가 가지고 있다는 근거는 없음.
- 따라서 딥러닝은 그냥 `데이터`로부터 `표현을 학습하는 수학 모델`임.



##### 딥러닝 학습과정

<img src="C:\Users\multicampus\Documents\s03p31c203\Document\KJW\Tensorflow study\Deep Learning with Python.assets\book2-2.jpg" alt="book2-2" style="zoom:67%;" />

<img src="C:\Users\multicampus\Documents\s03p31c203\Document\KJW\Tensorflow study\Deep Learning with Python.assets\book2-3.jpg" alt="book2-3" style="zoom:50%;" />

- 층을 거치면서 원본 이미지와는 점점 더 다른 표현으로 숫자 이미지가 변환 됌.
- 심층 신경망을 정보가 연속된 필터를 통과하면서 순도 높게(유용하게) 정제되는 다단계 정보 추출 작업으로 생각할 수 있음.
- 즉, 딥러닝이란 데이터 표현을 학습하기 위한 다단계 처리 방식을 말함.



##### 딥러닝, 머신러닝 비교

- `머신러닝`은 많은 `입력`과 `타깃(target)`의 샘플을 관찰하면서 입력(ex. 이미지)을 타깃(ex. 고양이 레이블)에 `매핑(mapping)`하는 것
- `딥러닝`은 이런 `입력-타깃 매핑`을 간단한 `데이터 변환기(층)를 많이 연결`하여 수행하는 것으로 머`신러닝의 한 분야`.



##### 딥러닝의 작동 원리

- 학습은 주어진 입력을 정확한 타닛에 매핑하기 위해 신경망의 모든 층에 있는 가중치 값을 찾는 것을 의미.
  - 이따금 가중치를 그 층의 파라미터라도 부름.
    - **이전 책에서 모두 매개변수로 표기하는 것을 이 책에서 파라미터로 표기했음. 그리고 파이썬 프로그램의 함수와 클래스에 전달할때 파라미터라고 표현하던 것을 매개변수로 표기함. 주의!! ** 
    - 정리하면서 매개변수로 바꿔 정리할까 했지만 내가 구분을 잘못하여 이해하는 데 오히려 방해가 될 수도 있을 거 같아 일단은 책 그대로 따라감.
  - 하지만 수천만개의 파라미터를 가지는 경우 모든 파라미터의 정확한 값을 찾는 것은 어려운 일임.(파라미터 하나의 값을 바꾸면 다른 모든 파라미터에 영향을 끼치기 때문임.)

<img src="C:\Users\multicampus\Documents\s03p31c203\Document\KJW\Tensorflow study\Deep Learning with Python.assets\book2-4.jpg" alt="book2-4" style="zoom: 33%;" />

- 가중치의 정확한 값을 찾는 것이 목표이므로 이때 출력이 기대하는 값보다 얼마나 벗어났는지 측정해야함.

<img src="C:\Users\multicampus\Documents\s03p31c203\Document\KJW\Tensorflow study\Deep Learning with Python.assets\book2-5.jpg" alt="book2-5" style="zoom:33%;" />

- 이를 담당하는 게 손실함수(loss function) 으로 신경망의 예측과 진짜 타깃(신경망의 출력으로 기대하는 값)의 차이를 점수로 계산하며 얼마나 잘 예측했는지 측정함.

  - 손실함수 == 목적함수(objective function) == 비용함수(cost function)
    - 비용함수는 모든 훈련 데이터에 대한 손실 함수의 합을 의미함.
    - 목적함수는 더 일반적인 용어로 최적화하기 위한 대상 함수를 의미함.
    - 보통 이 용어들을 구분하지 않고 혼용하여 사용하는 경우가 많음.

  

  <img src="C:\Users\multicampus\Documents\s03p31c203\Document\KJW\Tensorflow study\Deep Learning with Python.assets\book2-6.jpg" alt="book2-6" style="zoom:50%;" />

- 손실 점수를 피드백 신호로 사용하여 현재 샘플의 `손실 점수가 감소되는 방향`으로 `가중치 값을 조금씩 수정`하는 것임. 이런 수정 과정은 딥러닝 핵심 알고리즘인 `역전파(Back propagation) `알고리즘을 구현한 `옵티마이저(optimizer)`가 담당함.
  - 초기에는 네트워크의 가중치가 랜덤한 값으로 할당됌. 
  - 자연스럽게 출력은 기대한 것과 멀어지고 손실 점수가 높을 것.
  - 네트워크가 모든 샘플을 처리하면서 가중치가 조금씩 올바른 방향으로 조정되고 손실 점수가 감소함.
  - 이를 훈련 반복이라고 하며, 충분한 횟수만큼 반복하면(수천 개의 샘플에서 수 십번 반복하면) 손실 함수를 최소화하는 가중치 값을 산출함. 
  - 즉, `최소한의 손실을 내는 네트워크가 타깃에 가능한 가장 가까운 출력을 만드는 모델이 됌.`



### chapter2 시작하기전에: 신경망의 수학적 구성 요소

- MNIST 데이터셋을 활용하여 설명함.

  - MNIST는 28x28 크기의 흑백 손글씨 숫자 이미지임. 10개의 클래스를 가지고 있으며 6만개의 훈련 이미지와 1만개의 테스트 이미지로 구성되어 있음. 

  - MNIST 데이터셋은 Numpy(넘파이) 배열 형태로 케라스에 이미 포함되어 있음.

    - Numpy: 파이썬의 대표적인 다차원 배열 라이브러리, 케라스, 텐서플로, 사이킷런등에서 모두 이용.

    ```python 
    from tensorflow.keras.datasets import mnist
    
    (train_images, train_labels), (test_images, test_labels) = mnist.load_data()
    ```

    - 이미지는 `넘파이 배열로 인코딩`되어 있고 레이블은 0부터 9까지의 숫자 배열로 `이미지와 레이블은 일대일 관계`

- 신경망의 핵심 구성 요소는 일종의 `데이터 처리 필터`라고 생각할 수 있는 `층(layer)`임.층은 주어진 문제에 더 의미 있는 `표현(representation)`을 `입력된 데이터로부터 추출함`. 

- 대부분의 딥러닝은 간단한 층을 연결하여 구성되어 있고, 점진적으로 데이터를 정제하는 형태를 띠고 있음.딥러닝 모델은 데이터 정체 필터(층)가 연속 되어 있는 데이터 프로세싱을 위한 여과기와 같음.

- 학습시키는 과정 건너뜀.



#### 신경망을 위한 데이터 표현

- `텐서(Tensor)`: 다차원 넘파이 배열

  - 텐서는 `데이터를 위한 컨테이너`로 임의의 차원 개수를 가지는 행렬.
  - 텐서에서는 `차원(dimension)`을 종종 `축(axis)`라고 부름.
  -  텐서플로 포함 여러 딥러닝 라이브러리에서는 종종 `다차원 배열`을 `텐서`라고 부름. 이 책에서도 넘파이 배열도 텐서라고 부르지만 사실 파이썬 커뮤니티에서는 `넘파이 배열`을 `텐서라고 부르지 않음`.

- 스칼라(0D 텐서)

  - 하나의 숫자만 담고 있는 텐서를 `스칼라(== 스칼라 텐서, 0차원 텐서, 0D 텐서)` 라고 부름.

    - 넘파이에서는 float32, float64 타입의 숫자가 스칼라 텐서(==배열 스칼라(array scalar))
      - `keras의 부동 소수 기본 설정은 float32`
      - Numpy 자료형 참고: https://kongdols-room.tistory.com/53
      - IEEE 754 부동소수점 표현 방식: https://kama1204.tistory.com/entry/IEEE-%EC%8B%A4%EC%88%98-float

  - ndim 속성을 사용하면 넘파이 배열의 축 개수를 확인할 수 있음.

    ```python
    x = np.array(12)
    >>> x.ndim
    0
    ```

    - `스칼라 텐서`의 `축 개수는 0`임.(`ndim==0`)
    - 텐서의 축 개수를 `랭크(rank) `라고도 부름. 하지만 선형대수에서 행렬의 선형 독립 행이나 열을 나타내는 계수(rank)와는 다름.

- 벡터(1D 텐서)

  - 숫자의 배열을 `벡터(vector)` 또는 `1D` 텐서라고 부름. 1D 텐서는 딱 `하나의 축`을 가짐. 

    ```python
    x = np.array([12,13,14,15,16])
    >>> x.ndim
    1
    ```

  - 이 벡터는 5개의 원소를 가지고 있으므로 5차원 벡터라고 부름. 5D 벡터와 5D 텐서를 혼동하지 말것. 

    - 5D 벡터는 하나의 축을 따라 5개의 차원을 가진 것이고 5D 텐서는 5개의 축을 가진것(즉, 텐서의 축을 따라 여러 개의 차원을 가진 벡터가 놓일 수 있음.)

- 행렬(2D 텐서)

  - 벡터의 배열이 `행렬` 또는 `2D텐서`임. 

    ```python
    x = np.array([[12,13,14,15,16],
                 [32,33,34,35,36],
                 [72,73,74,75,76]])
    >>> x.ndim
    2
    ```

  - 행렬에는 `2개`의 축이 있음. (보통 `행(row)`와 `열(column)`이라고 부름.)

- 3D 텐서와 고차원 텐서

  - 위의 행렬들을 하나의 새로운 배열로 합치면 숫자가 채워진 `직육면체 형태`로 해석할 수 있는 `3D 텐서`가 만들어짐. 

    ```python
    x = np.array([[[12,13,14,15,16],
                 [32,33,34,35,36],
                 [72,73,74,75,76]],
                  
                 [[12,13,14,15,16],
                 [32,33,34,35,36],
                 [72,73,74,75,76]],
                  
                 [[12,13,14,15,16],
                 [32,33,34,35,36],
                 [72,73,74,75,76]]])
    
    >>> x.ndim
    3
    ```

  - 3D 텐서들을 하나의 배열로 합치면 `4D 텐서`를 만들 수 있음. 딥러닝에서는 보통 `0D에서 4D까지의 텐서`를 다루며 `동영상` 데이터를 다룰 경우 `5D 텐서`까지 가기도 함.

- 텐서의 핵심 속성 3가지

  - `축의 개수(랭크)`: 3D 텐서에는 3개의 축이 있고, 행렬에는 2개의 축이 있음. numpy 라이브러리에는 ndim속성에 저장되어 있음.
  - `크기(shape)`: 텐서의 각 축을 따라 얼마나 많은 차원이 있는지 나타낸 파이썬의 튜플(tuple)
    - 위에서의 `스칼라`는 `()`으로 크기가 없음., `벡터`의 크기는 `(5, )` `행렬`의 크기는 `(3, 5)`이고 `3D 텐서`의 크기는 `(3, 3, 5)`임. 
  - `데이터 타입`: 텐서에 포함된 데이터의 타입임. numpy에서는 dtype에 저장이 됌. float32, uint 8, float 64등이 될 수 있음.

  ```python 
  from tensorflow.keras.datasets import mnist
  
  (train_images, train_labels), (test_images, test_labels) = mnist.load_data()
  
  # train_images 배열의 축의 개수
  >>> print(train_images.ndim)
  3
  
  # 배열의 크기(shape)
  >>> print(train_images.ndim)
  (60000, 28, 28)
  
  # 데이터 타입
  >>> print(train_images.dtype)
  uint 8
  ```

  - 위 `train_images`는 `8비트 정수형 3D 텐서`임. 즉, 28 x 28 크기의 정수 행렬 6만개가 있는 배열로 각 행렬은 하나의 흑백 이미지고, 행렬의 각 원소는 0에서 255 사이의 값을 가짐.   



#### 배치 데이터

- 일반적으로 딥러닝에서 사용하는 모든 `데이터 텐서`의 `첫번째 축`은 `샘플 축(sample axis)` 또는 `샘플 차원(sample demension)`임. (인덱스가 0부터 시작하므로 0번째 축)

  - MNIST  예제에서는 숫자 이미지가 샘플.

  ```python
  # 11번째에서 100번째까지 (슬라이싱하여) 숫자 이미지 선택하여 (90, 28, 28)크기의 배열 만들기. 
  my_slice = train_images[10:100]
  >>> print(my_slice.shape)
  (90, 28, 28)
  
  # 위와 같은 크기의 배열 만듬. 표현만 다름.
  my_slice = train_images[10:100, 0:28, 0:28]
  >>> print(my_slice.shape)
  (90, 28, 28)
  
  my_slice = train_images[10:100, :, :]
  >>> print(my_slice.shape)
  (90, 28, 28)
  
  
  # 이미지의 오른쪽 아래 14x14픽셀을 선택할때
  my_slice = train_images[:, 14, 14:]
  my_slice = train_images[:, 7:-7, 7:-7] # 음수 인덱스도 사용할 수 있음.
  ```

  

- 딥러닝 모델은 한 번에 전체 데이터셋을 처리하지 않음. 그 대신 데이터를 작은 배치(batch)로 나눔. 

  - 배치 데이터를 다룰 때는 `첫번째 축`을(0번 축)을 `배치 축(batch axis)` 또는 `배치 차원(batch dimension)` 이라고 부름.

  ```python
  # 크기가 128인 배치
  batch = train_images[:128]
  
  # 그 다음 배치
  batch = train_images[128:256]
  
  # n번째 배치
  batch = train_images[128*n : 128*(n+1)]
  ```

- 텐서의 실제 사례

  - `벡터 데이터`: `(samples, features) `크기의 2D 텐서
  - `시계열 데이터` 또는 `시퀀스 데이터`: `(samples, timesteps, features)` 크기의 3D 텐서
  - `이미지 데이터`: `(samples, height, width, channels)` 또는 `(samples, channels, heigth, width)` 크기의 4D 텐서
  - `동영상 데이터`: `(samples, frames, height, width, channels)` 또는 `(samples, frames, channels, height, width)` 크기의 5D 텐서

- 이미지 데이터

  <img src="C:\Users\multicampus\Documents\s03p31c203\Document\KJW\Tensorflow study\Deep Learning with Python.assets\book2-7.jpg" alt="book2-7" style="zoom: 50%;" />

  - 이미지는 전형적으로 높이, 너비, 컬러 채널의 3차원으로 이뤄짐.
    - 흑백 이미지 한장은 하나의 컬러 채널만 가지고 있어 2D 텐서로  저장될 수 있지만 관례상 이미지 텐서는 항상 3D로 저장됌.
    - `256 x 256 크기`의 `흑백` 이미지에 대한 `128개`의 배치는  `(128, 256, 256, 1) 크기의 4D 텐서`로 `컬러` 이미지는  `(128, 256, 256, 3) 크기의 4D 텐서`로 저장됌.
  - 이미지 텐서의 크기를 지정하는 방식은 두가지(케라스에서는 두 형식 모두 지원.)
    - `channel-last`(채널 마지막 방식)
      - 텐서플로에서 주로 사용함.(여기서 말하는 텐서플로는 tf.keras 말고 tf만이용했을때인듯.)
      - (samples, height, width, color_depth)
    - `channel-first`(채널 우선 방식)
      - 씨아노에서 주로 사용함.
      - (samples, color_depth, heigt, width)
      - (128, 1, 256, 256 ) 또는 (128, 3, 256, 256)



#### 텐서의 크기 변환(Tensor reshaping)

- 신경망에 주입할 숫자 데이터를 전처리할 때 주로 사용.

  ```python 
  # ex. train_images = train_images.reshape((60000, 28*28))
  ```

- 텐서의 크기를 변환한다는 것은 특정 크기에 맞게 열과 행을 재배열한다는 뜻.

  - 크기가 변환된 텐서는 원래 텐서와 원소 개수가 동일함.

  ```python
  # 크기 변환 예시
  
  x = np.array[[0, 1],
               [2, 3],
               [4, 5]]
  >>> print(x.shape)
  (3,2)
  
  >>> x = x.reshpae((6, 1))
  [[0],
   [1],
   [2],
   [3],
   [4],
   [5]]
  
  >>> x= x.reshape((2, 3))
  [[0, 1, 2],
   [3, 4, 5]]
  ```



#### 신경망의 엔진: 그래디언트 기반 최적화

- `keras.layers.Dense(512, activation='relu')`
  - Dense 층은 2D 텐서를 입력으로 받고 입력 텐서의 새로운 표현인 또 다른 2D 텐서를 반환하는 함수.
  - 이 함수를 다르게 표현하면 `output= relu(dot(W, input) + b)` 으로 표현할 수 있음.
    - `W`는 `2D 텐서` 이고, `b`는 `벡터`로 둘 모두 `층의 속성`임. 자세하게 말하면 `Dense` 클래스 객체가 모델의 add() 메서드에 추가될 때  Dense 객체의 build() 메서드가 호출되면서 `가중치(커널,kernel) W`와 `편향 b`가 생성됌. 각각 `Dense 객체`의 `kernel` 과 `bias 인스턴스 변수`에 저장됌.
      - **여기서는 가중치(W) == kernel 인데 kernel은 여러가지 의미로 쓰임. 서포트 벡터 머신의 커널함수, CNN의 필터를 지칭함.**
    - output 을  얻는 과정에는 `3개의 텐서 연산`이 있음.  `입력 텐서`와 `텐서 W`사이의 `점곱(dot)`, 점곱의 결과인 `2D 텐서`와 `벡터 b 사이의 덧셈(+)`, 마지막으로 `relu 연산`임. (`여기서 relu(x)는 max(x, 0)임`). 
- 위와 같은 과정을 통해서 입력 데이터를 output으로 변환함. 이때 가중치(W)에는 훈련 데이터를 신경망에 노출시켜서 학습된 정보가 담겨 있음.
  - 초기에는 `가중치 행렬`이 작은 난수로 채워져 있음. 이를 `무작위 초기화(random initialization)` 단계라고 부름. 물론 `W`와 `b`가 난수 일때 `relu(dot(W, input) + b)`가 어떤 유용한 표현을 만들지 못함.
  - 하지만 `피드백 신호`에 기초하여 가중치가 점진적으로 조정됌.  이런 점진적인 조정이 `훈련(training)`임.
- `훈련 반복 루프(training loop)`
  - 훈련은 아래와 같은 훈련 반복 루프안에서 일어남.
    1. `훈련 샘플 x와 이에 상응하는 타깃 y의 배치를 추출함.`
    2. `x를 사용하여 네트워크를 실행하고(정방향 패스단계, forward pass), 예측 y_pred를 구함.`
    3. `y_pred와 y의 차이를 측정하여 이 배치에 대한 네트워크의 손실을 계산함.`
    4. `배치에 대한 손실이 조금 감소되도록 네트워크의 모든 가중치를 업데이트함.`
  - 1단계는 입출력 코드임. 2단계, 3단계는 몇 개의 텐서 연산을 적용한 것.
  - 4단계는 네트워크의 가중치를 업데이트 하는 단계인데, 개별적인 가중치 값이 있을때 값이 증가해야 할지 감소해야 할지, 또 얼마큼 업데이트해야 할지 어떻게 알수 있을까?
    - 네트워크 가중치 행렬의 원소를 모두 고정하고 관심 있는 하나만 다른 값을 적용하고 이 가중치의 초깃값이 0.3이라고 가정.
    - 배치 데이터를 정방향 패스에 통과시킨 후 네트워크의  손실이 0.5가 나옴. -> 이 가중치 값을 0.35로 변경하고 다시 정방향 패스했더니 손실이 0.6으로 증가함. -> 가중치 값을 0.25로 줄이고 정방향 패스했더니 손실이 0.4로 감소함. 따라서 가중치를 0.05 감소 시키며 업데이트 한 것이 손실을 줄이는 데 기여한 것으로 판단. 이런 식으로 네트워크의 모든 가중치에 반복 적용함.
    - 이런 접근 방식은 모든 가중치 행렬의 원소마다 두번의 정방향 패스를 계산해야함. 따라서 비효율적임.(보통의 경우에 수백만개의 많은 가중치가 존재함.)
    - 그래서 모든 연산이 `미분 가능(differentiable) `하다는 장점을 사용하여 네트워크 `가중치에 대한 손실`의 `그래디언트(gradient)`를 계산하는 것이 훨씬 효율적임. 
    - `그래디언트의 반대 방향으로 가중치를 이동하면 손실이 감소됌.`
- `변화율(==도함수, derivative)`
  - f(x) = a*x 일때 f'(x) = a임. 
  - a가 음수일 때 양수 x만틈 조금 이동하면 f(x)가 감소한다는 것을 의미. 또 a가 양수일 때 음수 x만큼 이동하면 f(x)가 감소됌. 여기서 a의 절댓값은 이런 증가나 감소가 얼마나 빠르게 일어나는지 알려줌.
  - 즉, f(x)를 감소 시키고 싶다면 x를 변화율(a)의 뱡향과 반대로 이동해야함.(여기서 방향은 부호라고 생각하면 될 듯.)
- `텐서 연산의 변화율: 그래디언트(f'(W), gradient)`
  - 그래디언트는 텐서 연산의 변화율, 다차원 입력(텐서)를 입력으로 받는 함수에 변화율 개념을 확장시킨것.
    - 이 책에서 입력 x에 대한 미분을 변화율이라고 하고 x를 상수로 생각하고 W에 대해 미분한 것을 그래디언트라고 표기함. (사실 둘이 같은 말이지 않나)
  - 입력 벡터 x, 행렬 W(==텐서==가중치), 타깃y, 손실 함수loss가 있다고 가정. W를 사용하여 타깃의 예측 y_pred를 계산하고 손실(y_pred와 타깃 y 사이의 오차)을 계산할 수 있음. 아래처럼 나타낼 수 있음.
    - y_pred = dot(W, x)
    - loss_value = loss(y_pred, y)
    - 만약에 입력데이터 x와 y가 고정되어 있다면 이 삼후는 W를 손실 값에 매핑하는 함수로 볼 수 있음.
    - 즉, loss_value = f(W)  (x랑 y가 고정되어 있다고 하니깐 변수는 W밖에 없고 loss_value는 W에 결정되므로 이렇게 치환됌.)
  - 위의 변화율에서 설명한 것과 동일한 방식을 적용하면 그래디언트의 반대방향으로 W를 움직이면 f(W) 값을 줄일 수 있음. 아래는 예시
    - `W1 = W0 - step*gradient(f)(W0)`: 이는 기울기가 작아지는 곡면의 낮은 위치로 이동된다는 의미.
    - `W1`은 업데이트 되어 `이동한 텐서`이고, `W0`은 `현재 텐서`, `gradient(f)(W0)`은` W0에서 함수 f(W)의 그래디언트`를 의미.
    - `step`은 스케일을 조정하기 위한 작은 값. (얼마나 이동할지를 조절하기 위한 상수인듯). `gradient(f)(W0)`는 W0에 아주 `가까이` 있을때 `기울기를 근사한 것`이므로 W0에서 너무 크게 벗어나지 않기 위해 스케일링 비율 step이 필요함.

#### 확률적 경사 하강법

- 미분 가능한 함수에서 `함수의 최솟값`은 `변화율(도함수)가 0인 지점`임.
- 따라서 위에 적용해 `loss_value가 최소`일때의 W를 찾기 위해  gradient(f)(W) = 0 을 풀면 되는 데, 이 식은 N개의 변수로 이뤄지 다항식임. (N은 네트워크의 가중치 개수)
- 실제 신경망에서 파라미터(다른책에서 매개변수)의 개수가 수천 개보다 적은 경우가 거의 없고 종종 수천만개이므로 해결하는 것이 어려움.
- 따라서 위 훈련 반복 루프 중 4번을 조금 바꿔 실시하게됌. (그래디언트의 반대 방향으로 가중치를 이동하면 손실이 감소되는 것을 이용.)
  1. `훈련 샘플 x와 이에 상응하는 타깃 y의 배치를 추출함.`
  2. `x를 사용하여 네트워크를 실행하고(정방향 패스단계, forward pass), 예측 y_pred를 구함.`
  3. `y_pred와 y의 차이를 측정하여 이 배치에 대한 네트워크의 손실을 계산함.`
  4. `네트워크의 파라미터에 대한 손실 함수의 그래디언트를 계산함.(역방향 패스, backward pass)`
  5. `그래디언트의 반대 방향으로 파라미터를 조금 이동시킴. 예를 들어 W -= step * gradient 처럼 하면 배치에 대한 손실이 조금 감소할 것.`
- 위의 과정을 `미니 배치 확률적 경사하강법`이라고 함. (`mini-batch stochastic gradient descent`)
  - `미니 배치 SGD`라고도 함. 
  - `확률적(stochastic)`: 각 배치 데이터가 무작위로 선택된다는 의미로 확률적이란 `무작위(random)`하다는 것의 과학적 표현임.

<img src="C:\Users\multicampus\Documents\s03p31c203\Document\KJW\Tensorflow study\Deep Learning with Python.assets\book2-9.jpg" alt="book2-9" style="zoom:50%;" />

- 위 그림은 1D 파라미터 공간에서 경사 하강법을 설명하고 있음.
  - 위 그림에서 보는 것처럼 step 값을 적절히 고르는 것이 중요함.
    - `step`이 너무 `작으면` 곡선을 따라 내려가는데 `너무 많은 반복`이 필요하고 `지역 최소값(local minimum)` 에 갇힐 수 있음.
    - `step`이 너무 `크면 `손실 함수 곡선에서 `완전히 임의의 위치`로 이동시킬 수 있음.
- 미니 배치 SGD의 종류
  - 하나의 샘플과 하나의 타닛을 뽑는 것. (이게 진정한 SGD라고 저자가 표현함.)
  - 다른 방식으로 가용한 모든 데이터를 사용하여 반복을 실행할 수 있는 이른 배치 SGD(batch SGD)라고 함.
    - 더 정확하게 업데이트 되지만 더 많은 비용이 듬.
  - 극단적인 두가지 방법의 효율적인 절충한은 적절한 크기의 미니 배치를 사용하는 것.

<img src="C:\Users\multicampus\Documents\s03p31c203\Document\KJW\Tensorflow study\Deep Learning with Python.assets\book2-10.jpg" alt="book2-10" style="zoom:50%;" />

- 그림 2-11은 1D 파라미터 공간에서 경사 하강법을 설명하고 있지만 실제는 매우 고차원 공간에서 경사하강법을 사용하게 됌. 

- 위의 그림 2-12는  2D 손실 함수의 표면을 따라 진행하는 경사 하강법을 시각화한것. 실제는 수만 수백만 차원이므로 신경망이 훈련되는 실제 과정을 시각화하기 어려움. 그러므로 저차원 표현으로 얻은 직관이 실전과 항상 맞지 않다는 것을 유념해야함.

  - 대표적으로 신경망 알고리즘이 지역 최솟값에 쉽게 갇힐 것으로 생각했지만 고차원 공간에서는 대부분 안장점으로 나타나고 지역 최솟값은 드뭄.

- 업데이트할 다음 가중치를 계산할 때 현재 그래디언트 값만 보지 않고 이전에 업데이트된 가중치를 여러가지 다른 방식으로 고려하는 SGD 종류가 많음.

  - 예를 들어 `모멘텀을 사용한 SGD`, `Adagrad`, `RMSProp` 임. 
  - 이것들을 `optimization method(최적화 방법)` 또는 `optimizer` 라고 부름.

- 모멘텀은 `SGD`에 있는 두가지 문제 점인 `수렴 속도`와 `지역 최솟값`을 해결해줌.

  <img src="C:\Users\multicampus\Documents\s03p31c203\Document\KJW\Tensorflow study\Deep Learning with Python.assets\book2-11-1604130651613.jpg" alt="book2-11" style="zoom:33%;" />

  - 위 그림은 네트워크의 파라미터 하나에 대한 손실 값의 곡선을 보여줌.
  - 일반적으로 모멘텀 값은 0.9를 사용함. 



### Chapter3 신경망 시작하기

- 부분적으로 정리함.

#### 이진 분류 예제

- `확률을 출력`하는 모델을 사용할때는 `crossentropy(크로스엔트로피)`가 최선의 선택임.
  - `binary_crossentropy`: 이진 분류 문제고 신경망의 출력이 확률일때 손실함수로 쓰임. 
- metrics로 정확도를 지정할때 `accuracy`와 `acc` 모두 가능함.
  - 이렇게 하면 자동으로 이진 분류에는 `metrics.binary_accuracy 함수` 사용되고, 다중 분류에는 `metrics.categorical_accuracy 함수`가 사용됌. 

```python
model.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
              metrics=['accuracy'])


history = model.fit(partial_x_train,
                    partial_y_train,
                    epochs=20,
                    batch_size=512,
                    validation_data=(x_val, y_val))
```

```python
history_dict = history.history
>>> history_dict.keys()
# dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy']) 가 출력됌.
```

- `model.fit()` 메서드는 `History 객체`를 반환함. 이 객체는 훈련하는 동안 발생하는 모든 정보를 담고 있는 딕셔너리인 history 속성을 가지고 있음. 

  - 위 딕셔너리는  훈련과 검증하는 동안 모니터링할 측정 지표당 하나씩 모두 네 개의 항목을 담고 있음.
  - `history.history` 딕셔러리에 들어 잇는 `측정 지표의 키`가 정확도일 경우 `accuracy` 가 키가 됌.  검증 세트의 경우에는 `val_` 의 접두사가 붙음.

  

```python
import matplotlib.pyplot as plt

#acc = history.history['accuracy']
#val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(acc) + 1)

# ‘bo’는 파란색 점을 의미
plt.plot(epochs, loss, 'bo', label='Training loss')
# ‘b’는 파란색 실선을 의미
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()
```

<img src="C:\Users\multicampus\Documents\s03p31c203\Document\KJW\Tensorflow study\Deep Learning with Python.assets\image-20201031181342937.png" alt="image-20201031181342937" style="zoom:80%;" />

- 훈련과 검증 손실 그래프 그리기임.
  - 그릴때 loss와 val_loss를 accuracy와 val_accuracy를 주면 훈련과 검증 정확도 그래프 그려짐.
  - 위 그래프 예시에서 epoch가 2이상부터는 over-fitting 훈련 데이터에 과도하게 최적화되어 훈련 데이터에 특화된 표현을 학습함. 따라서 훈련 세트 이외의 데이터에는 일반화되지 못함.

#### 다중 분류 문제

- 다중 분류 문제에서 손실함수는 주로 `categorical_crossentropy` (범주형 인코딩. 즉, 원-핫 인코딩 된 경우), `sparse_categorical_crossentropy`(정수로 인코딩된 경우)를 사용.



### chapter4 머신러닝의 기본 요소

#### 분류와 회귀에서 자주 사용되는 용어 정리

- `샘플` 또는 `입력`: 모델에 주입될 하나의 데이터 포인트.
- `예측` 또는 `출력`: 모델로부터 나오는 값.
- `타깃`: 정답. 외부 데이터 소스에 근거하여 모델이 완벽하게 예측해야 하는 값.
- `예측 오차` 또는 `손실 값`: 모델의 예측과 타깃 사이의 거리를 측정한 값.
- `클래스`: 분류 문제에서 선택할 수 있는 가능한 레이블의 집합. ex) 고양이와 강아지 사진을 분류할때 클래스는 고양이와 강아지 2개임.
- `레이블`: 분류 문제에서 클래스 할당의 구체적인 사례. 예를 들어 사진 #1234에 강아지 클래스가 들어있다고 표시 한다면 강아지는 사진 #1234의 레이블이 됌.
- `참 값(ground-truth)` 또는 `꼬리표(annotation)`: 데이터셋에 대한 모든 타깃. 일반적으로 사람에 의해 수집됌.
- `이진 분류`: 각 입력 샘플이 2개의 배타적인 범주로 구분되는 분류 작업
- `다중 분류(multiclass classification)`: 각 입력 샘플이 2개 이상의 범주로 구분되는 분류 작업. 손글씨 숫자 분류가 그 예시
- `다중 레이블 분류(multi-label, multiclass classification)`: 각 입력 샘플이 여러 개의 레이블에 할당될 수 있는 분류 작업. 예를 들어 하나의 이미지에 고양이와 강아지가 모두 들어 있을 때는 고양이 레이블과 강아지 레이블을 모두 할당해야함. 보통 이미지마다 레이블의 개수가 다름.
- `스칼라 회귀`: 타깃이 연속적인 스칼라 값인 작업. 주택 가격 예측이 좋은 예. 
- `벡터 회귀`: 타깃이 연속적인 값의 집합인 작업. 예를 들어 연속적인 값으로 이루어진 벡터로 이미지에 있는 경계 상자의 좌표 같은 여러 개의 값에 대한 회귀를 한다면 벡터 회귀임.
- `미니 배치` 또는 `배치`: 모델에 의해 동시에 처리되는 소량의 샘플 묶음(일반적으로 8개에서 128개 사이) 샘플 개수를 GPU의 메모리 할당이 용이하도록 2의 거듭제곱으로 하는 경우가 많음. 훈련할때 배치마다 한번씩 모델의 가중치에 적용할 경사 하강법 업데이트 값을 계산함.



#### 머신 러닝 모델 평가

- 머신 러닝의 목표는 `처음 본 데이터`에서 잘 작동하는 `일반화된 모델`을 얻는 것. 여기서 `over-fitting`은 주요 장애물임. 따라서 `신뢰할 수 있는 측정 방법`을 통해 `모델의 성능을 측정`하고 `과대적합`을 `피하고` `일반화`를 `최대화`하는 것이 중요함.
- `모델 평가의 핵심`은 `가용한 데이터를 항상 훈련, 검증, 테스트 3개의 세트로 나누는 것`.
  - 훈련 세트에서 모델을 훈련하고 검증 세트에서 모델을 평가함. 모델을 출시할 준비가 되면 테스트 세트에서 최종적으로 딱 한번 모델을 테스트함.
- 훈련 세트, 테스트 세트 2개만 사용하지 않는 이유
  - 모델을 개발할 때 항상 모델의 설정을 튜닝하기 때문임. 이때 층의 수나 층의 유닛 수를 선택하게 되는데 이런 것들을 `하이퍼파파라미터(Hyperparameter)`라고 부름. (네트워크의 가중치와 구분하기위해서 하이퍼파라미터라고 함.)
  - `정보 누설(information leak)` 개념이 핵심으로 검증 세트에 대한 모델 성능에 기반하어 모델의 하이퍼파라미터를 조정할 때마다 검증 데이터에 관한 정보가 모델로 새는 것임.
    - 한 번만 튜닝한다면 아주 적은 정보가 누설됌. 하지만 한 번 튜닝하고 검증 세트에 평가한 결과를 가지고 다시 모델을 조정하는 과정을 여러 번 반복하면, 검증 세트에 관한 정보를 모델에 의도치 않게 아주 많이 노출시키게 되는 것.
    - 결국, 검증 데이터에 맞추어 모델이 최적화됐고 검증 데이터에 잘 수행되는 모델이 만들어짐. 따라서 `검증 세트에 대한 성능을 기반으로 모델의 설정을 튜닝하면 검증 세트로 직접 훈련하지 않아도 빠르게 검증 세트에 over-fitting 될 수 있음.`
  - 따라서 `모델을 평가하기 위해 이전에 본 적 없는 완전히 다른 데이터 셋을 사용해야함.`(==테스트 세트)
    - 모델은 간접적으로라도 테스트 세트에 대한 어떤 정보도 얻어서는 안됌.
- 데이터를 훈련, 검증, 테스트 세트로 나누는 것은 간단해 보이지만 데이터가 적을 때 3가지 방법을 활용함.
  - `단순 홀드아웃 검증(hold-out validation)`
  - `K-겹 교차 검증(K-fold cross-validation)`
  - `셔플링을 사용한 반복 K-겹 교차 검증(iterated K-fold cross-validation)`
- `아래에서 설명하는 방식은 모두 테스트 세트를 이미 떼어 놓은 후를 가정한 것`

##### 단순 홀드아웃 검증

<img src="C:\Users\multicampus\Documents\s03p31c203\Document\KJW\Tensorflow study\Deep Learning with Python.assets\book2-12.jpg" alt="book2-12" style="zoom:67%;" />

- 데이터의 일정량을 테스트 세트로 떼어 놓음. 남은 데이터에서 훈련하고 검증함. 

  - 아마도 이전에 우리가 training, validation, test를 6:2:2로 나누던 방식이 이 방식인 것 같다.

- 홀드아웃 검증 코드 구현 예시

  <img src="C:\Users\multicampus\Documents\s03p31c203\Document\KJW\Tensorflow study\Deep Learning with Python.assets\book2-13.jpg" alt="book2-13" style="zoom:50%;" />

- 그림 4-1과 코드 4-1 는 `테스트 세트를 이미 떼어 놓은 후를 가정한 것`. 여기서처럼 직접 데이터를 나누기보다는 종종 `사이킷런`의 `train_test_split()` 함수를 사용하여 훈련, 검증, 테스트 세트로 나눔.

- 훈련-검증-튜닝을 반복하며 새로운 모델을 만들고, 최적의 하이퍼파라미터를 구한 후 `마지막 모델을 훈련시킬때는 훈련 데이터와 검증 데이터를 모두 사용하는 것이 중요함`. (꼭 필요한 과정인가?)

- 이 평가 방법의 단점은 데이터가 적을 때는 검증 세트와 테스트 세트의 `샘플이 너무 적어` 주어진 `전체 데이터를 통계적으로 대표하지 못할 수 있음`.

  - 다른 난수 초깃값으로 셔플링해서 데이터를 나누었을 때 모델의 성능이 매우 달라지면 이 문제임.
  - `K-겹 교차 검증과 반복 K-겹 교차 검증을 통해 해결 가능`.



##### K-겹 교차 검증

<img src="C:\Users\multicampus\Documents\s03p31c203\Document\KJW\Tensorflow study\Deep Learning with Python.assets\book2-14.jpg" alt="book2-14" style="zoom:50%;" />

- 여기서는 테스트 세트를 이미 떼어 놓은 후를 가정하고 진행함.

- 진행 방식

  - 데이터를 동일한 크기를 가진 K개 분할로 나눔. 
  - 각 분할 i에 대해 남은 K-1개의 분할로 모델을 훈련하고 분할 i에서 모델을 검증함. 
  - 최종 점수는 이렇게 얻은 K개의 점수를 평균함.

- 이 방식은 모델의 성능이 데이터 분할에 따라 편차가 클 때 도움이 됌.

- 구현 코드 예시

  <img src="C:\Users\multicampus\Documents\s03p31c203\Document\KJW\Tensorflow study\Deep Learning with Python.assets\book2-15.jpg" alt="book2-15" style="zoom:67%;" />

- `K-겹 교차 검증`은 `사이킷런`의 `cross_validate() 함수`를 사용하여 쉽게 구현할 수 있음. 단, 이 함수를 사용하려면 케라스 모델을 사이킷런과 호환되도록 `KerasClassifier` 나` KerasRegressor` 클래스로 모델을 감싸야함. (공식문서 한번보자.)

##### 셔플링을 사용한 반복 K-겹 교차 검증

- 비교적 가용 데이터가 적고 가능한 정확하게 모델을 평가하고자 할때 사용함. 
- 케글 경연에서 이 방법이 아주 크게 도움이 됌.
- K-겹 교차 검증을 여러 번 적용 하되 K개의 분할로 나누기 전에 매번 데이터를 무작위로 섞음.
  - 최종 점수는 모든 K-겹 교차 검증을 실행해서 얻은 점수의 평균으로 구함. 
- 하지만 섞는 작업을 하는 만큼 비용이 많이 듬. 
- 사이킷런의 RepeatedKFold(회귀) 와 RepeatedStratifiedKFold(분류) 클래스를 cross_validate() 함수에 적용하여 구현할 수 있음. 
  - https://bit.ly/2rSVwjB 참고.

##### 평가 방식을 선택할 때 유의할 점.

- 대표성이 있는 데이터
  - 훈련 세트와 테스트 세트가 주어진 데이터에 대한 `대표성`이 있어야함. 
  - 예를 들어 숫자 이미지가 클래스 순서대로 나열되어 있고 이 배열의 처음 80%를 훈련 세트로 나머지 20%를 테스트 세트로 만들면 훈련 세트에은 0~7만 담겨 있고 테스트 세트에는 8~9만 담겨 있음.
  - 이를 방지하기 위해서 훈련 세트와 테스트 세트로 나누기전에 데이터를 `무작위로 섞는 것이 일반적임`.
  - 하지만 `특정 클래스의 비율이 현저히 작다면` 무작위로 섞기 보다는` 클래스 비율이 고르게` 나눠지도록 해야함. 이를 `계층별(stratified) 분할`이라고 함. 
    - `사이킷런`의 `train_test_split() `함수는 `stratify 매개변수`로 타깃 레이블을 전달 받아 계층별 분할을 수행할 수 있음.

- 시간의 방향
  - 과거로부터 미래를 예측하려고 할때 (예를 들어 날씨나 주식 시세등) 데이터를 무작위로 섞으면 안됌. 
  - 섞으면 미래의 정보가 누설되기 때문 따라서 테스트 세트에 모든 미래 데이터가 있어야함.
- 데이터 중복
  - `한 데이터셋`에 어떤 `샘플이 중복되어 있는지 확인할 것`. 
  - 데이터를 섞고 훈련 세트와 검증 세트로 나눴을때 각각 하나씩 가지고 있어 훈련 테스트의 일부로 검증을 하게 되는 최악의 경우가 발생함.
  - 이렇게 섞이지 않아야 할 그룹을 지정하여 `교차 검증`을 하기 위해서는 `사이킷런`의 `GroupKFold 클래스`를 `cross_validate() 함수`에 적용할것.



- 참고 링크

https://github.com/rickiepark/introduction_to_ml_with_python/blob/1st_edition/05-model-evaluation-and-improvement.ipynb

