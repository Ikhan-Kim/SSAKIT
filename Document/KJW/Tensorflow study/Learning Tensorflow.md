# Learning Tensorflow(러닝 텐서플로)

- `이전 책 내용과 다른 설명(또는 더 쉬운 설명 ) 또는 코드 위주로 요약함.`

코드 예제나 연습 문제

- https://github.com/Hezi-Resheff/Oreilly-Learning-Tensorflow

### chapter4 합성곱 신경망

#### CNN 소개

- 공학 관점에서의 설명
  - 이미지 속에서 어떤 물체를 찾을때 이미지 내의 위치와는 무관하게 찾을 수 있어야 함. 이는 동일한 내용이 이미지의 다른 위치에서 발견될 수 있다는 이미지의 속성을 반영. 이를 `불변성(invariance)`이라고 함. 
  - (작은) 회전이 발생하거나 조명 조건이 변하더라도 유지되어야함. 따라서 객체 인식 시스템을 만들때 변환에 대한 불변성이 있어야 함. 그래서 이미지의 여러 다른 영역에 동일한 연산을 수행해 전체 공간에서 이미지의 동일한 특징을 계산함. (정확히 이해는 안됌.)
- 하나의 `정규화(regularization)` 메커니즘
  - 합성곱 구조를 하나의 정규화 메커니즘으로 볼 수 있음.
  - `정규화(regularization)`란 주어진 데이터에 over-fitting이 발생하는 것을 피하기 위해하는 작업. over-fitting은 학습에 사용되지 않은 데이터에 대해 일반화가 제대로 이뤄지지 않을 때 발생.

##### 완전 연결 신경망 (fully-connected)

<img src="C:\Users\multicampus\Documents\s03p31c203\Document\KJW\Tensorflow study\Learning Tensorflow.assets\book1-1603934696923.jpg" alt="book1" style="zoom:67%;" />

- 완전 연결 신경망과 합성곱 신경망의 차이점
  - 이어지는 계층간의 연결 패턴으로 완전 연결 신경에서 각각의 유닛은 앞의 계층의 모든 유닛과 연결되어 있음. 예시로 10개의 출력 유닛이 입력 이미지 픽셀에 모두 연결 되어 있음.

##### 합성곱 신경망(convolution)

- 합성곱 계층에서 각각의 유닛은 이전 계층에서 근접해 있는 몇 개의 유닛들에만 연결되어 있음. 
- 모든 유닛은 이전 계층에 동일한 방법으로 연결되어 같은 값의 가중치와 구조를 공유함. 이 사이에 합성곱 연산이 들어 있음. (이래서 합성곱 신경망이라는 이름을 가짐.)

- 합성곱 연산이란 이미지 전체에 가중치의 작은 윈도(window == filter, 필터라고도 부름.)를 적용하는 것.

  <img src="C:\Users\multicampus\Documents\s03p31c203\Document\KJW\Tensorflow study\Learning Tensorflow.assets\book2-1603935071393.jpg" alt="book2" style="zoom:33%;" />



#### MNIST 분류기

##### 합성곱 계층

- 위의 그림 4-2 참고

- `특징 맵(feature map)`: CNN모델에서는 합성곱 계층을 층층이 쌓아 올리는데 이들 `각 계층의 출력`을 말함.
  - 각 계층의 출력을 필터와 몇몇 연산을 적용한 결과인 '처리된 이미지'로 생각하는 것도 한 방법으로, 필터는 합성곱 필터를 나타내는 네트워크의 학습된 가중치인 W로 매개변수화 됌. 이는 그림 4-2에서의 작은 슬라이딩 윈도들의 가중치의 집합임
- `기울기 소실(gradient vanishing)`: 딥러닝에 사용했을때 계층이 늘어남에 따라 학습단계에서 역전파의 영향력이 급격히 줄어드는 것. 기존에는 `로지스틱 함수(== 시그모이드함수)`를 썼지만 기울기 소실 문제로 지금은 잘 쓰이지 않고 ReLU 등 다른 함수를 `활성화 함수`로 사용함. 

##### 풀링

- 합성곱 계층 다음에는 풀링을 하는 것이 일반적임.
- 풀링: 보통 각 특징 맵 내에서 어떤 지역적 집계 함수를 사용해 데이터의 크기를 줄이는 것을 뜻함.
- 풀링의 배경
  - 기술적인 이유
    - 풀링은 차례로 처리되는 데이터의 크기를 줄임. 이 과정을 통해 전체 매개변수의 수를 크게 줄일 수 있는데, 합성곱 계층 뒤에 완전 연결 계층을 사용할때 특히 그럼.
    - 풀링의 크기가 2x2 일때 풀링의 결과는 원본의 높이와 폭은 절반이 되고 크기는 1/4가 된다.
  - 이론적인 이유
    - 계산된 특징이 이미지 내의 위치의 사소한 변화에 영향을 받지 않기를 바래서 사용.
    - 예를 들어 이미지 우측에 위치한 눈을 찾는 특징은 카메라를 살짝 오른쪽으로 옮겨 눈이 이미지의 중앙 쪽에 위치하더라도 크게 영향을 받지 않아야 함. 
    - 따라서 눈을 찾는 특징을 공간적으로 모아내면 어떤 형태의 불변성을 찾아내서 이미지 간의 공간적 변화를 극복할 수 있는 모델을 만들 수 있음.

##### 드롭아웃

- 학습 중 값을 0으로 세팅해서 계층 내의 유닛 중 임의의 사전에 설정된 부분을 `꺼버리는 방식`.
- 드롭아웃된 뉴런은 각 연산마다 다르게 `무작위`로 선택되며, 네트워크가 드롭아웃 이후에도 표현을 학습하도록 강제함. 이 과정은 여러 네트워크의 `앙상블 학습`으로도 간주할 수 있음. 즉, 일반화를 강제화한다고 할 수 있음.
- 테스트 시점(추론)에서 사용할 시에는 드롭아웃 없이 네트워크 전체를 그대로 사용함.



##### 모델

<img src="C:\Users\multicampus\Documents\s03p31c203\Document\KJW\Tensorflow study\Learning Tensorflow.assets\book3-1603941119919.jpg" alt="book3" style="zoom:50%;" />

- 예제 합성곱 신경망의 구조
- 데이터의 공간적인 요소를 고려하기 위해서 합성곱을 사용함. 
- 이를 위해서 여기서는 두개의 합성곱과 풀링 계층을 만듬. 각각 5x5 합성곱과 32개 및 64개의 특징 맵을 가짐. 그 뒤에 1024개의 유닛을 가진 하나의 완전 연결 계층이 이어짐. 
  - 두 개의 합성곱과 풀링 계층에서 나오는 이미지의 크기는 7x7x64임. 원래 28x28 픽셀 이미지는 두 번의 풀링 과정에서 14x14의 크기로 축소된 후 다시 7x7의 크기로 축소됌.
  - 두번째 합성곱 계층에서 만들어진 특징 맵의 수는 64개.
- 완전 연결 계층에서는 더 이상 공간적 측면을 고려할 필요가 없으므로 완전 연결 계층을 적용하기 전에 이미지를 하나의 벡터 형태로 평탄화(flatten)함.
- 전체 매개변수는 `7 x 7 x 64 x 1024`로 320만개이다. 최댓값 풀링을 사용하지 않는 다면 이 수는 16배로 증가함.(`28 x 28 x 64 x 1024` = 5100만 개 )
- MNIST에 대해 정확도 향상 아이디어를 얻으려면 아래 링크 참고
  - http://yann.lecun.com/exdb/mnist/



#### CIFAR10 분류기

#### cifar10 데이터

- 이미지 파일
  - 32 x 32 크기의 이미지로 낮은 해상도에서도 어느정도 인식 가능한 하나의 완전한 객체가 가운데에 있음.

- 파이썬 버전의 데이터를 다운로드하고 데이터를 풀면
  - data_batch_1, data_batch_2, data_batch3, data_batch_4, data_batch_5
    - data_batch_x 파일은 학습 데이터가 포함된 직렬화된 데이터 파일.
  - test_batch
    - test_batch  파일은 테스트 데이터가 포함된 유사한 직렬화 파일
  - batches_meta
    - batches_meta 파일은 숫자에서 의미 레이블로의 매핑을 담고 있음.
  - readme.html
    - readme.html 파일은 cifar10 데이터 셋 웹 페이지 사본.
- ONE-HOT
  - 0-9까지(여기는 클래스가 10개이니깐) 정수인 레이블 값을 길이가 10인 벡터.
  - 이 벡터는 레이블 값에 해당하는 위치의 값이 1이고 나머지는 모두 0인 벡터임. 



#### 95%이상의 정확도를 얻는 방법을 알 수 있는 링크

- 여기 MNIST, CIFAR-10, CIFAR-100, STL-10, SVHN에 대한 분류 정확도 향상 방법.
  - http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130





### 7장 텐서플로 추상화와 간소화

- `추상화(abstraction)`: 원래 코드를 특정한 목적으로 일반화하여 기존 코드의 `위에 올라가는 코드의 계층`을 이르는 말.

##### 혼동 행렬(confusion matrix)

- 예측 메서드(`predict()`)를 사용하여 어떤 클래스가 가장 잘 식별하였으며 전형적인 오류의 유형이 무엇인지 등 모델의 성능을 분석할때 사용함.

- 여기서는 사이킷런 라이브러리를 활용

  - pip 이용해서 설치, 아나콘다에 이미 포함되어 있음. 따라서 확인 후 없으면 설치.

  ```python
  from sklearn.metrics import confusion_matrix
  # 여기서 만든 모델의 이름이 dnn임.
  y_pred = dnn.predict(x=x_test, as_iterable=False)
  class_names = ['0','1','2','3','4','5','6','7','8','9']
  cnf_matrix = confusion_matrix(y_test, y_pred)
  ```

  <img src="C:\Users\multicampus\Documents\s03p31c203\Document\KJW\Tensorflow study\Learning Tensorflow.assets\book4-1603952545149.jpg" alt="book4" style="zoom:50%;" />

  - 행은 `실제 숫자`에 대응되며 열은 `예측한 숫자`에 대응됌. 예를 들어 이 모델이 종종 `5를 3으로(13번 잘못 판단함.), 9를 4(16번)나 7(9번)로 잘못 분류`하고 있음을 볼 수 있음. 

